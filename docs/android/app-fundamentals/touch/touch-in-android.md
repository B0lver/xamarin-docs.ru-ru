---
title: Сенсорные технологии в Android
ms.prod: xamarin
ms.assetid: 405A1FA0-4EFA-4AEB-B672-F36307B9CF16
ms.technology: xamarin-android
author: davidortinau
ms.author: daortin
ms.date: 03/01/2018
ms.openlocfilehash: 960f75126fdfed770f79e0b4dad5641886eaf8ba
ms.sourcegitcommit: 2fbe4932a319af4ebc829f65eb1fb1816ba305d3
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/29/2019
ms.locfileid: "73024319"
---
# <a name="touch-in-android"></a>Сенсорные технологии в Android

Как и в iOS, Android создает объект, содержащий данные о физическом взаимодействии пользователя с экраном &ndash; `Android.View.MotionEvent`ном объекте. Этот объект содержит данные, такие как выполняемые действия, место касания, степень применения нажима и т. д. Объект `MotionEvent` разбивает перемещение на следующие значения:

- Код действия, описывающий тип движения, например начальное касание, сенсорный переход по экрану или сенсорный ввод.

- Набор значений осей, описывающих позицию `MotionEvent` и других свойств перемещения, таких как место касания, время касания и объем использованной нагрузки.
   Значения осей могут отличаться в зависимости от устройства, поэтому в предыдущем списке не описываются все значения осей.

Объект `MotionEvent` будет передан в соответствующий метод в приложении. Существует три способа реагирования на событие касания приложением Xamarin. Android:

- *Назначьте обработчик событий `View.Touch`* -класс `Android.Views.View` имеет `EventHandler<View.TouchEventArgs>`, к которому приложения могут назначать обработчик. Это типичное поведение .NET.

- *Реализация `View.IOnTouchListener`* -экземпляров этого интерфейса может быть назначена объекту представления с помощью представления. `SetOnListener` метод. Это функционально эквивалентно назначению обработчика событий для события `View.Touch`. Если существует общая или общая логика, которой может потребоваться много различных представлений, когда они затронуты, то будет более эффективным создание класса и реализация этого метода, чем Присвоение каждому представлению собственного обработчика событий.

- *Переопределите `View.OnTouchEvent`* — все представления в подклассе Android `Android.Views.View`. Когда представление затронуто, Android вызывает `OnTouchEvent` и передает ему объект `MotionEvent` в качестве параметра.

> [!NOTE]
> Не все устройства Android поддерживают сенсорные экраны. 

Добавление следующего тега в файл манифеста приведет к тому, что Google Play будет отображать приложение только для тех устройств, для которых включено касание:

```xml
<uses-configuration android:reqTouchScreen="finger" />
```

## <a name="gestures"></a>Жесты

Жест — это рисуемая вручную фигура на сенсорном экране. С жестом может быть один или несколько штрихов, каждый штрих, состоящий из последовательности точек, созданных в другой точке контакта с экраном. Android поддерживает множество различных типов жестов, от простых вставляет по экрану до сложных жестов, затрагивающих несколько касаний.

Android предоставляет пространство имен `Android.Gestures` специально для управления жестами и реагирования на них. В сердце всех жестов — Специальный класс с именем `Android.Gestures.GestureDetector`. Как следует из названия, этот класс будет прослушивать жесты и события на основе `MotionEvents`, предоставляемой операционной системой.

Чтобы реализовать средство обнаружения жестов, действие должно создать экземпляр класса `GestureDetector` и предоставить экземпляр `IOnGestureListener`, как показано в следующем фрагменте кода:

```csharp
GestureOverlayView.IOnGestureListener myListener = new MyGestureListener();
_gestureDetector = new GestureDetector(this, myListener);
```

Действие также должно реализовывать Онтаучевент и передавать Мотионевент детектору жестов. В следующем фрагменте кода приведен пример.

```csharp
public override bool OnTouchEvent(MotionEvent e)
{
    // This method is in an Activity
    return _gestureDetector.OnTouchEvent(e);
}
```

Когда экземпляр `GestureDetector` определяет нужный вам жест, он уведомляет действие или приложение, вызывая событие или через обратный вызов, предоставленный `GestureDetector.IOnGestureListener`.
Этот интерфейс предоставляет шесть методов для различных жестов:

- Вызывается, когда касание происходит, но не освобождается.

- *Онфлинг* — вызывается при возникновении вставляет и предоставляет данные в начале и в конце сенсорного ввода, вызвавшего событие.

- *Онлонгпресс* — вызывается при длительной нажатии.

- *OnScroll* -вызывается при возникновении события Scroll.

- *Оншовпресс* — вызывается после завершения работы, а событие перемещения или вверх не было выполнено.

- *Онсинглетапуп* — вызывается при возникновении одного касания.

Во многих случаях приложения могут заинтересовать только подмножество жестов. В этом случае приложения должны расширять класс Жестуредетектор. Симплеонжестурелистенер и переопределять методы, соответствующие интересующим Вас событиям.

## <a name="custom-gestures"></a>Пользовательские жесты

Жесты — это отличный способ взаимодействия пользователей с приложением. Интерфейсы API, которые мы видели до сих пор, достаточно для простых жестов, но могут доказать немного обременительным для более сложных жестов. Для облегчения работы с более сложными жестами Android предоставляет еще один набор API в пространстве имен Android. жесты, который упрощает некоторые косвенные нагрузки, связанные с пользовательскими жестами.

### <a name="creating-custom-gestures"></a>Создание настраиваемых жестов

Начиная с версии Android 1,6, пакет SDK для Android поставляется с предварительно установленным приложением в эмуляторе, называемом "Построитель жестов". Это приложение позволяет разработчику создавать предварительно определенные жесты, которые могут быть внедрены в приложение. На следующем снимке экрана показан пример построителя жестов.

[![снимок экрана построителя жестов с примерами жестов](touch-in-android-images/image11.png)](touch-in-android-images/image11.png#lightbox)

Улучшенную версию этого приложения, называемую инструментом жестов, можно найти Google Play. Инструмент жеста очень похож на построитель жестов, за исключением того, что он позволяет тестировать жесты после их создания. На следующем снимке экрана показан построитель жестов:

[![снимок экрана инструмента жестов с примерами жестов](touch-in-android-images/image12.png)](touch-in-android-images/image12.png#lightbox)

Инструмент "жест" является более полезным для создания настраиваемых жестов, так как он позволяет тестировать жесты при их создании и легко доступен с помощью Google Play.

Инструмент "жест" позволяет создать жест путем рисования на экране и назначения имени. После создания жестов они сохраняются в двоичном файле на SD-карте устройства. Этот файл необходимо получить с устройства, а затем упаковать с приложением в папке/Ресаурцес/рав. Этот файл можно получить из эмулятора с помощью Android Debug Bridge. В следующем примере показано, как скопировать файл из хранилища Galaxy в каталог ресурсов приложения:

```shell
$ adb pull /storage/sdcard0/gestures <projectdirectory>/Resources/raw
```

После получения файла он должен быть упакован в приложение внутри каталога/Ресаурцес/рав. Самый простой способ использовать этот файл жеста — загрузить файл в Жестурелибрари, как показано в следующем фрагменте кода:

```csharp
GestureLibrary myGestures = GestureLibraries.FromRawResources(this, Resource.Raw.gestures);
if (!myGestures.Load())
{
    // The library didn't load, so close the activity.
    Finish();
}
```

### <a name="using-custom-gestures"></a>Использование пользовательских жестов

Чтобы распознать пользовательские жесты в действии, к его макету должен быть добавлен объект Android. жестов. Жестуреоверлай. В следующем фрагменте кода показано, как программным способом добавить Жестуреоверлайвиев к действию:

```csharp
GestureOverlayView gestureOverlayView = new GestureOverlayView(this);
gestureOverlayView.AddOnGesturePerformedListener(this);
SetContentView(gestureOverlayView);
```

В следующем фрагменте кода XML показано, как добавить Жестуреоверлайвиев декларативно:

```xml
<android.gesture.GestureOverlayView
    android:id="@+id/gestures"
    android:layout_width="match_parent "
    android:layout_height="match_parent" />
```

`GestureOverlayView` имеет несколько событий, которые будут создаваться во время процесса рисования жеста. Наиболее интересное событие — `GesturePerformed`. Это событие возникает, когда пользователь завершает рисование своего жеста.

При возникновении этого события действие запрашивает у `GestureLibrary` попытку попробовать и сопоставить жест, который пользователь с одним из жестов создал с помощью средства жеста. `GestureLibrary` вернет список объектов прогноза.

Каждый объект прогноза содержит оценку и имя одного из жестов в `GestureLibrary`. Чем выше оценка, тем выше вероятность того, что жест, названный в прогнозе, соответствует жесту, нарисованному пользователем.
В целом, баллы ниже 1,0 считаются неплохими совпадениями.

В следующем коде показан пример сопоставления жеста:

```csharp
private void GestureOverlayViewOnGesturePerformed(object sender, GestureOverlayView.GesturePerformedEventArgs gesturePerformedEventArgs)
{
    // In this example _gestureLibrary was instantiated in OnCreate
    IEnumerable<Prediction> predictions = from p in _gestureLibrary.Recognize(gesturePerformedEventArgs.Gesture)
    orderby p.Score descending
    where p.Score > 1.0
    select p;
    Prediction prediction = predictions.FirstOrDefault();

    if (prediction == null)
    {
        Log.Debug(GetType().FullName, "Nothing matched the user's gesture.");
        return;
    }

    Toast.MakeText(this, prediction.Name, ToastLength.Short).Show();
}
```

После этого вы должны понимать, как использовать касания и жесты в приложении Xamarin. Android. Теперь давайте перейдем к пошаговому руководству и посмотрим все концепции в рабочем примере приложения.

## <a name="related-links"></a>Связанные ссылки

- [Запуск Android Touch (пример)](https://docs.microsoft.com/samples/xamarin/monodroid-samples/applicationfundamentals-touch-start)
- [Окончательное касание Android (пример)](https://docs.microsoft.com/samples/xamarin/monodroid-samples/applicationfundamentals-touch-final)
